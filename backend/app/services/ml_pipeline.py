import asyncio
import time
from pathlib import Path
import re
import json

import cv2
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from PIL import Image
from ultralytics import YOLO
from torchvision.models import mobilenet_v3_small

BASE_DIR = Path(__file__).resolve().parent.parent

class ModelPipeline:
    def __init__(self):
        self.models = {}
        self.device = torch.device('cpu')
        print(f"Using device: {self.device}")
        
        # ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹
        self.model_configs = {
            'yolo': {
                'path': str(BASE_DIR / 'model.pt'),
                'type': 'yolo'
            },
            'mobilenet_skin': {
                'path': str(BASE_DIR / 'mobilenet_skin.pth'),
                'num_classes': 3,
                'class_names': ['acne', 'freckles', 'healthy']
            },
            'mobilenet_age': {
                'path': str(BASE_DIR / 'mobilenet_age.pth'), 
                'num_classes': 6,
                'class_names': ['adult', 'baby', 'child', 'middle', 'pensioner', 'teenage']
            },
            'mobilenet_eyes_darkcircles': {
                'path': str(BASE_DIR / 'mobilenet_darkcircles.pth'),
                'num_classes': 3, 
                'class_names': ['darkcircles', 'healthy', 'light_darkcircles']
            },
            'mobilenet_eyes_pupils': {
                'path': str(BASE_DIR / 'mobilenet_eyes_pupils.pth'),
                'num_classes': 3,
                'class_names': ['conjunctivitis', 'healthy', 'yellowness']
            },
            'mobilenet_general': {
                'path': str(BASE_DIR / 'mobilenet_general.pth'),
                'num_classes': 2,
                'class_names': ['edema', 'healthy'] 
            }
        }
        
        self.transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])

    async def load_yolo_model(self):
        """ÐÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð°Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° YOLO Ð¼Ð¾Ð´ÐµÐ»Ð¸"""
        print("ðŸ”„ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° YOLO Ð¼Ð¾Ð´ÐµÐ»Ð¸...")
        try:
            self.models['yolo'] = YOLO(self.model_configs['yolo']['path'])
            print("âœ… YOLO Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°")
        except Exception as e:
            print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ YOLO: {e}")
            raise

    async def load_mobilenet_model(self, model_name):
        """ÐÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð°Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° MobileNet Ð¼Ð¾Ð´ÐµÐ»Ð¸"""
        print(f"ðŸ”„ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° {model_name}...")
        try:
            config = self.model_configs[model_name]
            
            model = mobilenet_v3_small(weights=None)
            model.classifier[3] = nn.Linear(
                model.classifier[3].in_features, 
                config['num_classes']
            )
            
            # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð²ÐµÑÐ°
            checkpoint = torch.load(config['path'], map_location=self.device)
            if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                model.load_state_dict(checkpoint['state_dict'])
            else:
                model.load_state_dict(checkpoint)
            
            model.to(self.device)
            model.eval()
            
            self.models[model_name] = {
                'model': model,
                'class_names': config['class_names']
            }
            print(f"âœ… {model_name} Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°")
            
        except Exception as e:
            print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ {model_name}: {e}")
            raise

    async def load_all_models(self):
        """ÐÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð°Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð²ÑÐµÑ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹"""
        print("ðŸš€ ÐÐ°Ñ‡Ð°Ð»Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð²ÑÐµÑ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹...")
        start_time = time.time()
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð´Ð»Ñ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
        tasks = []
        tasks.append(self.load_yolo_model())
        
        mobilenet_models = [
            'mobilenet_skin', 'mobilenet_age', 'mobilenet_eyes_darkcircles',
            'mobilenet_eyes_pupils', 'mobilenet_general'
        ]
        
        for model_name in mobilenet_models:
            tasks.append(self.load_mobilenet_model(model_name))
        
        # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð²ÑÐµ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾
        await asyncio.gather(*tasks)
        
        loading_time = time.time() - start_time
        print(f"ðŸŽ‰ Ð’ÑÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ Ð·Ð° {loading_time:.2f} ÑÐµÐºÑƒÐ½Ð´")
        return True
    
    def yolo_detect_faces(self, image_path):
        """Ð”ÐµÑ‚ÐµÐºÑ†Ð¸Ñ Ð»Ð¸Ñ† Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ YOLO"""
        print("ðŸ” YOLO: Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ñ Ð»Ð¸Ñ†...")
        results = self.models['yolo'].predict(
            source=image_path,
            conf=0.7,
            imgsz=640,
            save=False
        )
        
        faces = []
        image = cv2.imread(image_path)
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        for i, result in enumerate(results):
            for j, box in enumerate(result.boxes):
                if int(box.cls[0].item()) == 0:  # ÐºÐ»Ð°ÑÑ 'face'
                    x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
                    confidence = box.conf[0].item()
                    
                    # Ð’Ñ‹Ñ€ÐµÐ·Ð°ÐµÐ¼ Ð»Ð¸Ñ†Ð¾ Ñ Ð·Ð°Ð¿Ð°ÑÐ¾Ð¼
                    margin = 20
                    h, w = image_rgb.shape[:2]
                    x1 = max(0, x1 - margin)
                    y1 = max(0, y1 - margin)
                    x2 = min(w, x2 + margin)
                    y2 = min(h, y2 + margin)
                    
                    face_crop = image_rgb[y1:y2, x1:x2]
                    
                    if face_crop.size > 0:
                        faces.append({
                            'crop': face_crop,
                            'bbox': (x1, y1, x2, y2),
                            'confidence': confidence
                        })
                        print(f"   âœ… ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¾ Ð»Ð¸Ñ†Ð¾ {len(faces)} (ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ: {confidence:.3f})")
        
        return faces

    async def process_with_mobilenet(self, model_name, face_crop):
        """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð²Ñ‹Ñ€ÐµÐ·Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð»Ð¸Ñ†Ð° MobileNet Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ"""
        try:
            model_data = self.models[model_name]
            model = model_data['model']
            class_names = model_data['class_names']
            
            # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð² PIL Image Ð¸ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼Ñ‹
            pil_image = Image.fromarray(face_crop)
            input_tensor = self.transform(pil_image).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                outputs = model(input_tensor)
                probabilities = torch.nn.functional.softmax(outputs, dim=1)
                confidence, predicted_class = torch.max(probabilities, 1)
            
            result = {
                'model': model_name,
                'predicted_class': predicted_class.item(),
                'class_name': class_names[predicted_class.item()],
                'confidence': confidence.item(),
                'all_probabilities': probabilities.cpu().numpy()[0]
            }
            
            return result
            
        except Exception as e:
            print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð² {model_name}: {e}")
            return {
                'model': model_name,
                'error': str(e)
            }

    async def process_face_pipeline(self, face_crop):
        """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð»Ð¸Ñ†Ð° Ñ‡ÐµÑ€ÐµÐ· Ð²ÐµÑÑŒ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½ MobileNet Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹"""
        print("ðŸ”„ Ð—Ð°Ð¿ÑƒÑÐº Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½Ð° ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸...")
        
        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸
        pipeline_order = [
            'mobilenet_skin',
            'mobilenet_age', 
            'mobilenet_eyes_darkcircles',
            'mobilenet_eyes_pupils',
            'mobilenet_general'
        ]
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð´Ð»Ñ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸
        tasks = []
        for model_name in pipeline_order:
            task = self.process_with_mobilenet(model_name, face_crop)
            tasks.append(task)
        
        # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð²ÑÐµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾
        results = await asyncio.gather(*tasks)
        
        return results

    async def process_image(self, image_path):
        """ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ"""
        print(f"\nðŸŽ¯ ÐÐ°Ñ‡Ð°Ð»Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ: {Path(image_path).name}")
        start_time = time.time()
        
        # Ð¨Ð°Ð³ 1: Ð”ÐµÑ‚ÐµÐºÑ†Ð¸Ñ Ð»Ð¸Ñ† YOLO
        faces = self.yolo_detect_faces(image_path)
        
        if not faces:
            print("âŒ Ð›Ð¸Ñ†Ð° Ð½Ðµ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ñ‹")
            return None
        
        all_results = []
        
        # Ð¨Ð°Ð³ 2: ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð»Ð¸Ñ†Ð° Ñ‡ÐµÑ€ÐµÐ· Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½
        for i, face in enumerate(faces):
            print(f"\nðŸ‘¤ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð»Ð¸Ñ†Ð° {i+1}/{len(faces)}...")
            
            # Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½ Ð´Ð»Ñ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ Ð»Ð¸Ñ†Ð°
            face_results = await self.process_face_pipeline(face['crop'])
            
            result = {
                'face_id': i + 1,
                'bbox': face['bbox'],
                'detection_confidence': face['confidence'],
                'classification_results': face_results
            }
            
            all_results.append(result)
        
        total_time = time.time() - start_time
        print(f"\nâœ… ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð° Ð·Ð° {total_time:.2f} ÑÐµÐºÑƒÐ½Ð´")
        
        return all_results

    def print_results(self, results):
        if not results:
            return "ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°"
        
        analysis_text = ""
        
        for result in results:
            analysis_text += f"ðŸ‘¤ Ð›Ð˜Ð¦Ðž {result['face_id']}\n"
            analysis_text += f"ðŸ“ BBox: {result['bbox']}\n"
            analysis_text += f"ðŸŽ¯ Ð£Ð²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¸: {result['detection_confidence']:.3f}\n\n"
            
            for classification in result['classification_results']:
                if 'error' in classification:
                    analysis_text += f"âŒ {classification['model']}: {classification['error']}\n"
                else:
                    analysis_text += f"âœ… {classification['model']}:\n"
                    analysis_text += f"   ðŸ·ï¸  ÐšÐ»Ð°ÑÑ: {classification['class_name']}\n"
                    analysis_text += f"   ðŸ“Š Ð£Ð²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ: {classification['confidence']:.3f}\n"
                    
                    # Ð¢Ð¾Ð¿ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ
                    probs = classification['all_probabilities']
                    class_names = self.models[classification['model']]['class_names']
                    
                    prob_indices = [(prob, idx) for idx, prob in enumerate(probs)]
                    prob_indices.sort(reverse=True, key=lambda x: x[0])
                    
                    analysis_text += "   ðŸ“ˆ Ð¢Ð¾Ð¿ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ:\n"
                    for prob, idx in prob_indices[:3]:
                        is_predicted = (idx == classification['predicted_class'])
                        marker = "â­" if is_predicted else "  "
                        analysis_text += f"      {marker} {class_names[idx]}: {prob:.3f}\n"
                    
                    analysis_text += "\n"
            
            analysis_text += "â”€" * 50 + "\n\n"
        
        return analysis_text

    async def parse_llm_response(self, llm_answer: str) -> dict:
        """
        ÐŸÐ°Ñ€ÑÐ¸Ñ‚ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚ LLM Ð¸ Ð¸Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²ÑƒÑŽ Ñ‡Ð°ÑÑ‚ÑŒ Ð¸ JSON Ñ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼Ð¸
        """
        try:
            # Ð˜Ñ‰ÐµÐ¼ JSON Ð² Ñ‚ÐµÐºÑÑ‚Ðµ (Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð² Ñ€Ð°Ð·Ð½Ñ‹Ñ… Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð°Ñ…)
            json_pattern = r'\{[^{}]*"[^"]*"[^{}]*\}'
            json_matches = re.findall(json_pattern, llm_answer)
            
            parameters = {}
            text_content = llm_answer
            
            if json_matches:
                # Ð‘ÐµÑ€ÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ð¹ JSON (ÑÐºÐ¾Ñ€ÐµÐµ Ð²ÑÐµÐ³Ð¾ ÑÐ°Ð¼Ñ‹Ð¹ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹)
                json_str = json_matches[-1]
                text_content = llm_answer.replace(json_str, '').strip()
                
                # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ JSON Ð¾Ñ‚ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ñ‹Ñ… Ð»Ð¸ÑˆÐ½Ð¸Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²
                json_str = re.sub(r'^```json\s*|\s*```$', '', json_str).strip()
                
                try:
                    parameters = json.loads(json_str)
                except json.JSONDecodeError:
                    # Ð•ÑÐ»Ð¸ Ð½Ðµ Ð¿Ð°Ñ€ÑÐ¸Ñ‚ÑÑ, Ð¿Ð¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð¿Ð¾Ñ‡Ð¸ÑÑ‚Ð¸Ñ‚ÑŒ ÐµÑ‰Ðµ
                    json_str = re.sub(r'[\n\r\t]', '', json_str)
                    parameters = json.loads(json_str)
            
            return {
                "analysis_text": text_content,
                "parameters": parameters
            }
            
        except Exception as e:
            print(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ð° LLM: {e}")
            # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð²ÐµÑÑŒ Ñ‚ÐµÐºÑÑ‚ ÐºÐ°Ðº analysis_text, ÐµÑÐ»Ð¸ Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ñ€Ð°ÑÐ¿Ð°Ñ€ÑÐ¸Ñ‚ÑŒ
            return {
                "analysis_text": llm_answer,
                "parameters": {}
            }

pipeline = ModelPipeline()

def get_pipeline() -> ModelPipeline:
    return pipeline
